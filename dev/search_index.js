var documenterSearchIndex = {"docs":
[{"location":"cli/#CLI-tool","page":"CLI tool","title":"CLI tool","text":"A small tutorial-style CLI script is included at bin/tensorlogic.\n\nRun it from the package root:\n\njulia --project=. bin/tensorlogic -d Person:100 \"tall(x)\"\njulia --project=. bin/tensorlogic --output-format dot \"knows(x,y)\" > graph.dot\njulia --project=. bin/tensorlogic --output-format json \"knows(x,y)\" > graph.json\njulia --project=. bin/tensorlogic --validate -d Person:2 \"forall x:Person. knows(x,y) -> likes(x,y)\"\n\nThis CLI compiles expressions, can validate them, and exports stats/DOT/JSON. It does not execute numeric backends; use eval_dense programmatically for that.","category":"section"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Rule-programs","page":"API","title":"Rule programs","text":"","category":"section"},{"location":"api/#Expression-language","page":"API","title":"Expression language","text":"","category":"section"},{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/#TensorLogic.parse_tensorlogic","page":"API","title":"TensorLogic.parse_tensorlogic","text":"Parse TensorLogic rule surface syntax into IRProgram.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.TLContext","page":"API","title":"TensorLogic.TLContext","text":"Runtime context holding interned constants and predicate relations.\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.run!","page":"API","title":"TensorLogic.run!","text":"Run a program in a context. Returns ctx after running.\n\nKeyword arguments:\n\nmaxiters: maximum forward-chaining iterations (rounds)\nstop: :fixpoint (default) or :maxiters\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.relation_tuples","page":"API","title":"TensorLogic.relation_tuples","text":"Return decoded tuples for a predicate as NTuple{N,Symbol} values.\n\nIf the predicate is unknown, returns an empty vector.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.parse_tlexpr","page":"API","title":"TensorLogic.parse_tlexpr","text":"Parse tutorial-style expression language into TLExpr.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.compile_graph","page":"API","title":"TensorLogic.compile_graph","text":"Compile a TLExpr into a TLGraph (a DAG for analysis/export).\n\nThe graph form is used for:\n\nDOT export (export_dot)\nJSON export (export_json)\ntooling/inspection\n\nIt does not change semantics; it is an analysis representation.\n\nCompile a TLExpr into a TLGraph (DAG) for analysis and export.\n\nUse export_dot and export_json on the returned graph.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.export_dot","page":"API","title":"TensorLogic.export_dot","text":"Export a TLGraph to GraphViz DOT format as a string.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.export_json","page":"API","title":"TensorLogic.export_json","text":"Serialize a TLGraph to a JSON string.\n\nKeyword arguments:\n\nindent: indentation level passed to JSON3\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.validate_expr","page":"API","title":"TensorLogic.validate_expr","text":"Validate an expression against declared domains and (optionally) predicate signatures.    Validate a TLExpr against a CompilerContext.\n\nReturns a ValidationReport.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.eval_dense","page":"API","title":"TensorLogic.eval_dense","text":"Evaluate a tutorial-style expression to a dense LabeledTensor.    Evaluate a TLExpr to a LabeledTensor.\n\nUse inputs to supply predicate tensors, and config to select semantics.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.resolve_backend","page":"API","title":"TensorLogic.resolve_backend","text":"Resolve a backend selector into a concrete DenseBackend.\n\nSelectors:\n\n:auto      -> choose the best available backend\n:broadcast -> broadcast backend\n:omeinsum  -> OMEinsum backend (requires optional extension)\n\nYou may also pass a concrete backend instance.\n\n\n\n\n\n","category":"function"},{"location":"api/#TensorLogic.IRProgram","page":"API","title":"TensorLogic.IRProgram","text":"A program consisting of a list of ground facts and rules.\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.Atom","page":"API","title":"TensorLogic.Atom","text":"Predicate applied to arguments.\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.Rule","page":"API","title":"TensorLogic.Rule","text":"A rule head :- body... (or equivalent bracket rule).\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.CompilerContext","page":"API","title":"TensorLogic.CompilerContext","text":"Context for expression-language compilation and validation.\n\nStores domain sizes and predicate signatures.\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.CompilationConfig","page":"API","title":"TensorLogic.CompilationConfig","text":"Semantic configuration for interpreting connectives and quantifiers.\n\nSee soft_differentiable(), fuzzy_godel(), and probabilistic() constructors.\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.LabeledTensor","page":"API","title":"TensorLogic.LabeledTensor","text":"Dense tensor paired with axis symbols (one per dimension).\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.ValidationReport","page":"API","title":"TensorLogic.ValidationReport","text":"Validation report returned by validate_expr.\n\nFields:\n\nok (Bool)\nerrors (Vector{String})\nwarnings (Vector{String})\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.GreedyPlanner","page":"API","title":"TensorLogic.GreedyPlanner","text":"A simple greedy planner that chooses pairwise contractions to keep intermediates small.\n\n\n\n\n\n","category":"type"},{"location":"api/#TensorLogic.plan_contraction","page":"API","title":"TensorLogic.plan_contraction","text":"Greedy plan: repeatedly contract the pair with smallest estimated intermediate size.\n\n\n\n\n\n","category":"function"},{"location":"design/#Design","page":"Design","title":"Design","text":"TensorLogic.jl is intentionally split into two independent subsystems, plus a small shared “core”:\n\nRule programs (sparse relational semantics)\nExpression language (dense tensor semantics)\nCore utilities (Dictionaries.jl helpers and shared types)\n\nThis separation lets you use either subsystem without pulling in the other.","category":"section"},{"location":"design/#1)-Rule-programs-(sparse-fixpoint-engine)","page":"Design","title":"1) Rule programs (sparse fixpoint engine)","text":"Parse rules to an IRProgram\nStore relations as tuple sets (SparseRelation{N})\nForward-chain monotone rules to a fixpoint (run!)\n\nThis is the backend you want when relations would be extremely sparse: it avoids materializing huge mostly-zero dense tensors.","category":"section"},{"location":"design/#2)-Expression-language-(dense)","page":"Design","title":"2) Expression language (dense)","text":"Parse tutorial-style expressions to a TLExpr AST\nValidate (domains, arities, variable binding discipline)\nCompile to a DAG (TLGraph) for analysis/export (DOT/JSON)\nEvaluate to labeled tensors (LabeledTensor(data, axes))\n\nThe dense evaluator is semantics-driven: a CompilationConfig selects the concrete interpretation of connectives and quantifiers (e.g. product/max semantics, sum/mean reductions).","category":"section"},{"location":"design/#Dense-execution-backends","page":"Design","title":"Dense execution backends","text":"The dense evaluator supports an optional execution backend layer:\n\nBroadcastBackend (default): axis alignment + broadcasting + reductions\nOMEinsumBackend (optional via Julia extension): for the hot path   exists v. (P1 & P2 & ... & Pk) under product AND + sum/mean EXISTS, execute as a tensor contraction and (optionally) optimize contraction order via OMEinsumContractionOrders\n\nWhen an expression does not match the hot-path preconditions, evaluation automatically falls back to BroadcastBackend.","category":"section"},{"location":"design/#Source-layout-(Julia-oriented-separation-of-concerns)","page":"Design","title":"Source layout (Julia-oriented separation of concerns)","text":"src/\n  core/            # infrastructure (Dictionaries.jl wrappers)\n  logic/           # rule programs: IR + parser + sparse engine\n    sparse/\n  expr/            # expression language: AST + parser + validation + eval\n  tensor/          # dense tensor helpers + backends + planners\n  cli/             # command-line interface\n\nGuiding rule: parsing builds data; semantics interprets data; execution runs semantics.","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Sparse-rule-programs-(relations-as-tuples)","page":"Examples","title":"Sparse rule programs (relations as tuples)","text":"See:\n\nexamples/bracket_ancestor.jl\nexamples/datalog_ancestor.jl\nexamples/sparse_triangle.jl\nexamples/sparse_path_with_filter.jl","category":"section"},{"location":"examples/#Expression-language-(compile/validate/export/eval)","page":"Examples","title":"Expression language (compile/validate/export/eval)","text":"See:\n\nexamples/expr_validate_export.jl\nexamples/dense_eval_strategies.jl\nexamples/dense_eval_omeinsum_optional.jl (requires OMEinsum.jl)","category":"section"},{"location":"rule_programs/#Rule-programs-(sparse)","page":"Rule programs (sparse)","title":"Rule programs (sparse)","text":"The rule-program path is designed for huge mostly-zero relations, without materializing dense tensors.","category":"section"},{"location":"rule_programs/#Syntax","page":"Rule programs (sparse)","title":"Syntax","text":"Facts:\n\nParent[Alice,Bob].\nParent(Alice,Bob).\n\nRules:\n\nAncestor[x,y] = Parent[x,y].\nAncestor[x,z] = Ancestor[x,y] * Parent[y,z].\n\n# or datalog:\nAncestor(x,y) :- Parent(x,y).\nAncestor(x,z) :- Ancestor(x,y), Parent(y,z).","category":"section"},{"location":"rule_programs/#Execution","page":"Rule programs (sparse)","title":"Execution","text":"using TensorLogic\n\nprog = parse_tensorlogic(src)\nctx = TLContext()\nrun!(ctx, prog; maxiters=50)\n\nrelation_tuples(ctx, :Ancestor)","category":"section"},{"location":"rule_programs/#Notes","page":"Rule programs (sparse)","title":"Notes","text":"The engine computes the least fixpoint under forward chaining.\nRelations are stored as sets of tuples of interned ids.","category":"section"},{"location":"tri_map/#Tri-map-of-the-module","page":"Tri-map","title":"Tri-map of the module","text":"This page gives three non-overlapping representations of the package. Together they act as a tri-directional map: you can navigate the system by concepts, by math/semantics, or by computation/dataflow and always end up at the same implementation points.\n\n","category":"section"},{"location":"tri_map/#Representation-A:-Conceptual-architecture-(concerns-and-invariants)","page":"Tri-map","title":"Representation A: Conceptual architecture (concerns and invariants)","text":"","category":"section"},{"location":"tri_map/#Layer-1:-Languages","page":"Tri-map","title":"Layer 1: Languages","text":"Two surface syntaxes, two parsers, one shared IR spirit:\n\nRule programs (Datalog-like + bracket syntax)\nfacts and rules over relations (predicates as sets of tuples)\nevaluated via a monotone fixpoint engine (sparse backend)\nExpression language\nlogical connectives, quantifiers, predicate calls\ncan be compiled to a graph, validated, exported (DOT/JSON), and evaluated (dense backend)\n\nInvariants:\n\nparsing is total over the supported grammar and fails with ArgumentError on invalid input\nparsing does not allocate “execution state”; it only produces IR / AST structures","category":"section"},{"location":"tri_map/#Layer-2:-Semantics","page":"Tri-map","title":"Layer 2: Semantics","text":"Two semantics families:\n\nRelational semantics (sparse):\nrelations are sets of tuples\nrule bodies are joins; heads are projections\nevaluation is monotone and converges to a least fixpoint (within maxiters)\nTensor semantics (dense):\npredicate calls are tensors over domains\nconnectives and quantifiers are interpreted by a CompilationConfig (“strategy”)\n\nInvariants:\n\nsparse semantics is idempotent and monotone under set union\ndense semantics is shape-safe: axes represent logical variables; operations preserve axis meaning","category":"section"},{"location":"tri_map/#Layer-3:-Execution-backends","page":"Tri-map","title":"Layer 3: Execution backends","text":"sparse execution: tuple-relations and join/projection\ndense execution: broadcast backend by default; optional OMEinsum backend for a specific hot path\n\nInvariants:\n\nbackend selection never changes which expression is being evaluated, only how it is executed\noptional backends are loaded via Julia extensions and are not required for the core package\n\n","category":"section"},{"location":"tri_map/#Representation-B:-Mathematical-semantics-(algebra)","page":"Tri-map","title":"Representation B: Mathematical semantics (algebra)","text":"","category":"section"},{"location":"tri_map/#Rule-programs-(sparse)","page":"Tri-map","title":"Rule programs (sparse)","text":"Let a relation R of arity n be a subset R ⊆ D₁ × … × Dₙ.\n\nA rule of the form:\n\nH(t₁,…,tₙ) = B₁(...) * ... * Bk(...)\n\nis interpreted as:\n\nJoin the body relations on shared variables\nFilter on constants\nProject the resulting tuples to the head variables\nUnion into H\n\nRepeated application yields the least fixpoint (when monotone).","category":"section"},{"location":"tri_map/#Expression-language-(dense)","page":"Tri-map","title":"Expression language (dense)","text":"Let predicates be tensors P : D_{a₁} × ... × D_{a_r} → [0,1] (typical TensorLogic setting).\n\nA CompilationConfig specifies:\n\nand_kind (e.g. product)\nor_kind (e.g. max)\nnot_kind (e.g. 1-x)\nimply_kind\nexists_reduce_kind (sum / mean / max)\nforall_reduce_kind\n\nEvaluation maps each expression to a labeled tensor (data, axes) where axes are variable symbols.\n\nKey property used for optimization:\n\nfor product AND and sum/mean EXISTS, exists v. (P₁ & ... & Pk) can be executed as a tensor contraction where v is omitted from the output indices (contract-and-reduce).\n\n","category":"section"},{"location":"tri_map/#Representation-C:-Computational-pipelines-(dataflow)","page":"Tri-map","title":"Representation C: Computational pipelines (dataflow)","text":"","category":"section"},{"location":"tri_map/#Pipeline-1:-sparse-rules","page":"Tri-map","title":"Pipeline 1: sparse rules","text":"String → parse_tensorlogic → IRProgram → run! → TLContext(relations) → relation_tuples\n\nHot spots:\n\njoin/projection in logic/sparse/engine.jl\nrelation storage in logic/sparse/relations.jl","category":"section"},{"location":"tri_map/#Pipeline-2:-expressions-(analysis-tools)","page":"Tri-map","title":"Pipeline 2: expressions (analysis tools)","text":"String → parse_tlexpr → TLExpr → (compile_graph, validate_expr, export_dot/export_json)\n\nHot spots:\n\nAST builder (expr/parser.jl)\nvalidator (expr/validate.jl)","category":"section"},{"location":"tri_map/#Pipeline-3:-expressions-(dense-execution)","page":"Tri-map","title":"Pipeline 3: expressions (dense execution)","text":"TLExpr + CompilerContext + inputs → eval_dense → LabeledTensor\n\nExecution choices:\n\nbroadcast backend (axis alignment + broadcasting + reductions)\noptional OMEinsum backend:\ndetect the EXISTS-conjunction hot path\nplan/order contraction\n(optionally) optimize order using OMEinsumContractionOrders when installed\ncache optimized contraction code","category":"section"},{"location":"#TensorLogic.jl","page":"Home","title":"TensorLogic.jl","text":"TensorLogic.jl provides two complementary layers:\n\nRule programs (sparse): parse Datalog-like / bracket rules to an IR, then run a monotone fixpoint over relations-as-tuples.\nExpression language (dense): parse tutorial-style expressions, compile to a DAG, validate, export DOT/JSON, and (optionally) evaluate densely with configurable semantics.\n\nThe project is optimized for Julia v1.12 and avoids generalized Einstein summation in the core implementation.\n\nSee docs/ABSTRACT_MODEL.md in the repository root for the refactoring contract.","category":"section"},{"location":"#How-to-navigate","page":"Home","title":"How to navigate","text":"If you want to understand how the package is structured, start with Tri-map.\nIf you want the precise semantics, see Abstract model.\nIf you want to use the package, see Examples and API.","category":"section"},{"location":"abstract_model/#Abstract-model","page":"Abstract model","title":"Abstract model","text":"This is the single-source semantic specification for the package. For a complementary architectural and computational view, see Tri-map.\n\nThis document is the single source of truth for the meaning and shape of TensorLogic.jl. Any refactor / rewrite is correct iff it preserves the contracts below.","category":"section"},{"location":"abstract_model/#1.-Two-user-facing-entry-points","page":"Abstract model","title":"1. Two user-facing entry points","text":"TensorLogic.jl deliberately offers two independent entry points:","category":"section"},{"location":"abstract_model/#A.-Rule-programs-(sparse,-fixpoint)","page":"Abstract model","title":"A. Rule programs (sparse, fixpoint)","text":"Surface: Datalog-like and/or bracket “tensor equation” rules.\nCompilation: parse_tensorlogic(::String) -> IRProgram.\nExecution: run!(::TLContext, ::IRProgram) computes the least fixpoint under monotone forward chaining.\nStorage: relations are sets of tuples of interned ids (no dense tensor materialization).","category":"section"},{"location":"abstract_model/#B.-Expression-programs-(dense,-tutorial-style)","page":"Abstract model","title":"B. Expression programs (dense, tutorial-style)","text":"Surface: tutorial-style expressions with connectives and quantifiers: & | ! -> exists forall\nCompilation: parse_tlexpr(::String) -> TLExpr, then compile_graph(::TLExpr) -> TLGraph.\nEvaluation: eval_dense(::TLExpr, ::CompilerContext; inputs, config, consts) evaluates to a LabeledTensor (dense semantics, no generalized einsum).\n\nThese two entry points share terminology but do not need to share execution engines.","category":"section"},{"location":"abstract_model/#2.-Core-data-model","page":"Abstract model","title":"2. Core data model","text":"","category":"section"},{"location":"abstract_model/#2.1-Sparse-fixpoint-engine","page":"Abstract model","title":"2.1 Sparse fixpoint engine","text":"TLContext maintains:\nobj2id :: Dictionary{Symbol,Int} and id2obj :: Vector{Symbol}\nrels  :: Dictionary{Symbol,Any} mapping predicate -> SparseRelation{N}\nSparseRelation{N} stores:\ntuples :: Set{NTuple{N,Int}}\n\nInvariant S1 (interning): object_symbol(ctx, intern!(ctx, s)) == s.\n\nInvariant S2 (arity): each predicate name maps to exactly one arity. Conflicts are errors.\n\nSemantics S3: run! is monotone and returns the least fixpoint for a finite domain.","category":"section"},{"location":"abstract_model/#2.2-Dense-expression-compiler","page":"Abstract model","title":"2.2 Dense expression compiler","text":"CompilerContext maintains:\ndomains :: Dictionary{Symbol,Int}\npred_domains :: Dictionary{Symbol,Vector{Symbol}} (predicate signature)\nTLExpr AST supports:\nPred, AndExpr, OrExpr, NotExpr, ImplyExpr, ExistsExpr, ForallExpr.\nCompilationConfig selects a connective semantics (strategy):\nsoft_differentiable, hard_boolean, fuzzy_godel, fuzzy_product, fuzzy_lukasiewicz, probabilistic.\n\nInvariant D1 (shape): for each Pred(name,args) the input tensor rank equals arity, and each axis length equals the declared domain size for that argument.\n\nInvariant D2 (axes): dense evaluation outputs a LabeledTensor whose axes are exactly the free variables of the expression.\n\nSemantics D3 (no einsum): dense evaluation uses explicit axis alignment, pointwise operators, and reductions.","category":"section"},{"location":"abstract_model/#3.-Non-functional-constraints","page":"Abstract model","title":"3. Non-functional constraints","text":"","category":"section"},{"location":"abstract_model/#Julia-1.12-performance-contract","page":"Abstract model","title":"Julia 1.12 performance contract","text":"no global mutable state in hot paths\nstable types in inner loops\ntuple rows are NTuple of Int\noptional dense utilities must remain isolated from sparse fixpoint path","category":"section"},{"location":"abstract_model/#Collections-contract","page":"Abstract model","title":"Collections contract","text":"No Base.Dict in src/.\nUse Dictionaries.Dictionary for associative maps.","category":"section"},{"location":"abstract_model/#Error-model","page":"Abstract model","title":"Error model","text":"syntax errors must be explicit and local\narity/signature mismatches are errors\nvalidation produces a structured report (ValidationReport)","category":"section"},{"location":"abstract_model/#4.-Extensibility-points","page":"Abstract model","title":"4. Extensibility points","text":"add new surface syntax by compiling to IRProgram (rule path) or TLExpr (expression path)\nadd new strategies by extending the operator mapping functions in compiler/strategies.jl\nadd sparse weighted semantics via a new relation type, keeping the run! monotone shape","category":"section"},{"location":"abstract_model/#Dense-backends-and-planners","page":"Abstract model","title":"Dense backends and planners","text":"The expression-language subsystem separates semantics (logical connectives and quantifiers under a CompilationConfig) from execution.\n\nDenseBackend selects a concrete contraction engine for a specific hot path: exists v. (P1 & P2 & ... & Pk) where each Pi is a predicate call, AND uses product semantics, and exists reduces by sum or mean.\nContractionPlanner selects a pairwise contraction order (default GreedyPlanner) used by backends that do not optimize contraction order automatically.\nWhen the preconditions are not met, evaluation falls back to the broadcast backend (axis alignment + broadcasting).\n\nCorrectness note: this optimization assumes tensors represent non-negative truth-values (typical in TensorLogic-style models), so that reordering products and sum/mean reductions does not change results beyond floating-point rounding.","category":"section"},{"location":"abstract_model/#OMEinsum-execution-details","page":"Abstract model","title":"OMEinsum execution details","text":"If backend=:omeinsum is selected and the OMEinsum extension is active, the evaluator will:\n\nbuild a direct einsum that omits the quantified variable from the output indices (contract-and-reduce),\noptionally optimize order using OMEinsumContractionOrders when installed,\ncache optimized contraction codes keyed by index pattern and domain sizes.","category":"section"},{"location":"expression_language/#Expression-language-(dense)","page":"Expression language (dense)","title":"Expression language (dense)","text":"This path mirrors the tutorial’s logical operations, quantifiers, and strategy mapping.","category":"section"},{"location":"expression_language/#Syntax","page":"Expression language (dense)","title":"Syntax","text":"Predicates: knows(x,y)\nConnectives: & | ! ->\nQuantifiers: exists y:Person. expr, forall x:Person. expr\n\nVariables are identifiers starting with _ or a lowercase letter. Constants are identifiers starting with an uppercase letter.","category":"section"},{"location":"expression_language/#Compile-to-a-graph","page":"Expression language (dense)","title":"Compile to a graph","text":"expr = parse_tlexpr(\"exists y:Person. knows(x,y) & likes(x,y)\")\ng = compile_graph(expr)\n\ndot = export_dot(g)\njson = export_json(g)","category":"section"},{"location":"expression_language/#Validate","page":"Expression language (dense)","title":"Validate","text":"ctx = CompilerContext()\nadd_domain!(ctx, :Person, 100)\n# optionally: declare_predicate!(ctx, :knows, [:Person,:Person])\n\nrep = validate_expr(expr, ctx)\nrep.ok\nrep.errors\nrep.warnings","category":"section"},{"location":"expression_language/#Dense-evaluation","page":"Expression language (dense)","title":"Dense evaluation","text":"ctx = CompilerContext()\nadd_domain!(ctx, :Person, 2)\ndeclare_predicate!(ctx, :knows, [:Person,:Person])\n\ninputs = Dictionary{Symbol,Any}()\nset!(inputs, :knows, [0.0 1.0; 0.2 0.3])\n\nout = eval_dense(parse_tlexpr(\"exists y:Person. knows(x,y)\"), ctx; inputs=inputs)\nout.axes   # [:x]\nout.data   # Vector{Float64}","category":"section"},{"location":"expression_language/#Backends","page":"Expression language (dense)","title":"Backends","text":"eval_dense supports optional dense backends.\n\nDefault: backend=:auto (currently :broadcast)\nOptional: backend=:omeinsum (requires installing OMEinsum.jl)\n\nExample:\n\nusing TensorLogic, OMEinsum\n\nexpr = parse_tlexpr(\"exists y:Person. knows(x,y) & likes(x,y)\")\nout = eval_dense(expr, ctx; inputs=inputs, backend=:omeinsum)\n\nNotes:\n\nThe OMEinsum backend is activated only for expressions of the form exists v:Dom. (P1 & P2 & ... & Pk) where each Pi is a predicate call.\nIt requires product-style AND semantics and uses einsum-style contraction only for sum/mean reductions (it will fall back otherwise).\nAll other expressions use the broadcast backend.\n\nExample with backend options:\n\nusing TensorLogic, OMEinsum\n\nbe = TensorLogic.OMEinsumBackend(; ntrials=8, slice_target=nothing)\nout = eval_dense(expr, ctx; inputs=inputs, backend=be)","category":"section"}]
}
